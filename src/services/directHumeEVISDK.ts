/**
 * Direct Frontend-to-Hume EVI Connection using proper SDK approach
 * Uses @humeai/voice with createSocketConfig pattern
 */

import { VoiceClient, createSocketConfig, getAudioStream, base64ToBlob } from '@humeai/voice';
import { EVIWebAudioPlayer } from 'hume';

export interface JobContext {
  title: string;
  company: string;
  duration: string;
  location: string;
  description: string;
  skills: string[];
  software: string[];
  aiSuggestedSkills?: string[];
  aiSuggestedSoftware?: string[];
  experienceId?: string;
}

export interface EVISessionData {
  sessionId: string;
  interviewId: string;
  configId: string;
  status: 'connecting' | 'connected' | 'interviewing' | 'completed' | 'error';
}

export interface EVIMessage {
  type: 'user_message' | 'assistant_message' | 'audio_output' | 'user_interruption' | 'error';
  content?: string;
  timestamp: string;
  emotions?: any;
}

class DirectHumeEVI {
  private voiceClient: VoiceClient | null = null;
  private audioPlayer: EVIWebAudioPlayer | null = null;
  private mediaStream: MediaStream | null = null;
  private recorder: MediaRecorder | null = null;

  // Session management
  private currentSession: EVISessionData | null = null;
  private isConnecting: boolean = false;
  private isRecording: boolean = false;
  private isEnding: boolean = false;
  private transcript: EVIMessage[] = [];

  // Event handlers
  private messageHandlers: Map<string, Function> = new Map();
  private audioEndHandlerAttached: boolean = false;

  // Backend API base URL
  private baseUrl: string;

  constructor() {
    // Use environment variable for API URL - CLEAN ARCHITECTURE
    this.baseUrl = `${import.meta.env.VITE_API_URL || 'http://localhost:3001/api'}/interview`;

    console.log('üéØ DirectHumeEVI SDK initialized with CLEAN baseUrl:', this.baseUrl);
  }

  /**
   * Connect to existing EVI session with config ID and access token
   * Used for recruiter screening on profile pages
   */
  async connectToExistingConfig(
    configId: string,
    accessToken: string,
    sessionId: string
  ): Promise<EVISessionData> {
    if (this.isConnecting || this.currentSession) {
      throw new Error('Interview already active or connecting');
    }

    // Reset audio player state for new interview
    this.audioPlayer = null;
    this.audioEndHandlerAttached = false;

    this.isConnecting = true;
    console.log('üöÄ Connecting to existing EVI config...');

    try {
      // Get audio stream first for proper SDK initialization (CRITICAL FIX)
      console.log('üé§ Getting audio stream for SDK...');
      this.mediaStream = await getAudioStream();
      console.log('‚úÖ Audio stream obtained for SDK');

      // Create socket configuration with existing config
      console.log('üîå Creating socket configuration with existing config...');
      const socketConfig = createSocketConfig({
        auth: {
          type: 'accessToken',
          value: accessToken
        },
        configId: configId,
        debug: false
      });

      // Create VoiceClient with socket config
      this.voiceClient = new VoiceClient(socketConfig);

      // Set up event handlers
      this.setupVoiceClientHandlers();

      // Connect to Hume
      await this.voiceClient.connect();
      console.log('‚úÖ Connected to Hume EVI via existing config');

      this.currentSession = {
        sessionId,
        interviewId: sessionId,
        configId,
        status: 'connecting'
      };

      return this.currentSession;

    } catch (error) {
      this.isConnecting = false;
      console.error('‚ùå Error connecting to existing config:', error);
      throw error;
    }
  }

  /**
   * Start a new EVI interview session
   */
  async startInterview(
    userId: string,
    interviewType: 'job_experience' | 'contextual' | 'work_style',
    jobContext?: JobContext
  ): Promise<EVISessionData> {
    if (this.isConnecting || this.currentSession) {
      throw new Error('Interview already active or connecting');
    }

    // Reset audio player state for new interview
    this.audioPlayer = null;
    this.audioEndHandlerAttached = false;

    this.isConnecting = true;
    console.log('üöÄ Starting direct EVI interview with SDK...');

    try {
      // Step 1: Get config ID from backend
      console.log('üìã Creating interview config via backend...');
      console.log('üåê Request URL:', `${this.baseUrl}/create-config`);
      console.log('üì¶ Request body:', { userId, interviewType, jobContext });
      
      let configResponse;
      try {
        configResponse = await fetch(`${this.baseUrl}/create-config`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            userId,
            candidateData: { id: userId }, // Simplified for clean architecture
            interviewType, 
            jobContext,
            experienceId: jobContext?.experienceId 
          })
        });
      } catch (fetchError) {
        console.error('‚ùå Network error during config fetch:', fetchError);
        throw new Error(`Network error: ${fetchError.message}`);
      }

      console.log('üì° Config response status:', configResponse.status);
      console.log('üì° Config response ok:', configResponse.ok);

      if (!configResponse.ok) {
        const errorData = await configResponse.json().catch(() => ({ error: 'Unknown error' }));
        console.error('‚ùå Config creation failed:', errorData);
        throw new Error(errorData.error || `HTTP ${configResponse.status}`);
      }

      const { configId, sessionId, interviewId } = await configResponse.json();
      console.log('‚úÖ Config created:', configId);

      // Step 2: Get access token
      console.log('üîë Getting access token...');
      console.log('üåê Token request URL:', `${this.baseUrl}/get-token`);
      console.log('üì¶ Token request body:', { sessionId });
      
      const tokenResponse = await fetch(`${this.baseUrl}/get-token`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sessionId })
      });

      console.log('üì° Token response status:', tokenResponse.status);
      console.log('üì° Token response ok:', tokenResponse.ok);

      if (!tokenResponse.ok) {
        const tokenErrorData = await tokenResponse.json().catch(() => ({ error: 'Failed to get access token' }));
        console.error('‚ùå Token request failed:', tokenErrorData);
        throw new Error(tokenErrorData.error || 'Failed to get access token');
      }

      const { accessToken } = await tokenResponse.json();

      // Debug: Log token details
      console.log('üîê Access Token received:', {
        tokenPreview: accessToken.substring(0, 50) + '...',
        tokenLength: accessToken.length,
        tokenType: typeof accessToken,
        startsWithEyJ: accessToken.startsWith('eyJ')
      });

      // Step 3: Create socket configuration
      console.log('üîå Creating socket configuration...');
      console.log('üÜî Using Config ID:', configId);

      const socketConfig = createSocketConfig({
        auth: {
          type: 'accessToken',  // Use accessToken, not apiKey
          value: accessToken
        },
        configId: configId,
        debug: true // Enable debug temporarily to see what's happening
      });

      // Step 4: Get audio stream first for proper SDK initialization
      console.log('üé§ Getting audio stream for SDK...');
      this.mediaStream = await getAudioStream();
      console.log('‚úÖ Audio stream obtained for SDK');

      // Step 5: Create VoiceClient with socket config and audio stream
      this.voiceClient = new VoiceClient(socketConfig);

      // Set up event handlers
      this.setupVoiceClientHandlers();

      // Connect to Hume (no parameters)
      console.log('üîó Attempting WebSocket connection to Hume...');
      try {
        // Connect without parameters - audio will be sent manually after connection
        await this.voiceClient.connect();
        console.log('‚úÖ Connected to Hume EVI via SDK');
      } catch (wsError: any) {
        console.error('üî¥ WebSocket Connection Error:', {
          message: wsError.message,
          stack: wsError.stack,
          error: wsError,
          configId: configId,
          tokenLength: accessToken?.length
        });

        // Test raw WebSocket as diagnostic
        console.log('üß™ Testing raw WebSocket connection as diagnostic...');
        const testWsUrl = `wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}&config_id=${configId}`;
        const testWs = new WebSocket(testWsUrl);

        testWs.onopen = () => {
          console.log('‚úÖ Raw WebSocket connected successfully! Issue is with SDK configuration.');
          testWs.close();
        };

        testWs.onerror = (e) => {
          console.error('‚ùå Raw WebSocket also failed:', e);
          console.log('üîç This indicates an authentication or network issue, not SDK issue');
        };

        // Try to provide more specific error message
        if (wsError.message?.includes('401') || wsError.message?.includes('Unauthorized')) {
          throw new Error('Authentication failed: Invalid or expired access token');
        } else if (wsError.message?.includes('404')) {
          throw new Error('Configuration not found: The EVI config may have expired or been deleted');
        } else if (wsError.message?.includes('network')) {
          throw new Error('Network error: Unable to connect to Hume servers');
        }
        throw wsError;
      }

      this.currentSession = {
        sessionId,
        interviewId,
        configId,
        status: 'connecting'  // Will be updated to 'connected' in 'open' event
      };

      // Audio is already connected to SDK
      return this.currentSession;

    } catch (error) {
      this.isConnecting = false;
      console.error('‚ùå Error starting interview:', error);
      throw error;
    }
  }

  /**
   * Set up event handlers for VoiceClient
   */
  private setupVoiceClientHandlers(): void {
    if (!this.voiceClient) return;

    console.log('üîß Setting up VoiceClient event handlers...');

    // Connection opened
    this.voiceClient.on('open', async () => {
      console.log('‚úÖ WebSocket connection opened');
      this.isConnecting = false;

      // Start audio recording after connection is established
      try {
        await this.startAudioRecording();
        console.log('‚úÖ Audio recording started');
      } catch (error) {
        console.error('‚ùå Failed to start audio recording:', error);
        this.emit('error', error);
      }

      console.log('üîä Audio player will be initialized when first AI audio is received');

      // Update session status
      if (this.currentSession) {
        this.currentSession.status = 'connected';
      }

      this.emit('connected');
    });

    // Handle messages
    this.voiceClient.on('message', (message: any) => {
      this.handleVoiceMessage(message);
    });

    // Connection closed
    this.voiceClient.on('close', () => {
      console.log('üîå Connection closed');
      this.cleanup();
      this.emit('disconnected');
    });

    // Connection error
    this.voiceClient.on('error', (error: any) => {
      console.error('‚ùå Connection error:', error);
      this.emit('error', error);
    });
  }

  /**
   * Handle messages from VoiceClient
   */
  private async handleVoiceMessage(message: any): Promise<void> {
    console.log('üì• Message:', message.type);
    
    // Log timeout-related events for debugging
    if (message.type === 'timeout_warning' || message.type === 'session_ended') {
      console.log('‚è∞ Timeout event received:', message.type, message);
    }

    switch (message.type) {
      case 'user_message':
        console.log('üë§ User:', message.message?.content);
        this.transcript.push({
          type: 'user_message',
          content: message.message?.content || '',
          timestamp: new Date().toISOString(),
          emotions: message.models?.prosody?.scores
        });
        this.emit('user_message', message);
        break;

      case 'assistant_message':
        console.log('ü§ñ Assistant:', message.message?.content);
        // Check if this is a timeout-related message
        const isTimeoutMessage = message.message?.content?.includes('one minute left') || 
                                message.message?.content?.includes('reached the end of our interview time');
        if (isTimeoutMessage) {
          console.log('‚è∞ This is a timeout-related assistant message');
        }
        this.transcript.push({
          type: 'assistant_message',
          content: message.message?.content || '',
          timestamp: new Date().toISOString(),
          emotions: message.models?.prosody?.scores
        });
        this.emit('assistant_message', message);
        break;

      case 'audio_output':
        console.log('üîä Audio output received');
        // Initialize audio player on first audio output if not already done
        if (!this.audioPlayer) {
          try {
            console.log('üîä Initializing audio player for first AI audio...');
            await this.initializeAudioPlayer();
            console.log('‚úÖ Audio player initialized successfully');
          } catch (error) {
            console.error('‚ùå Failed to initialize audio player:', error);
            this.emit('audio_output', message); // Still emit the event even if player fails
            break;
          }
        }
        await this.playAudioOutput(message);
        this.emit('audio_output', message);
        break;

      case 'user_interruption':
        console.log('‚úã User interruption');
        if (this.audioPlayer) {
          this.audioPlayer.stop();
        }
        this.emit('user_interruption', message);
        break;

      case 'error':
        console.error('‚ùå Error message:', message);
        this.emit('evi_error', message);
        break;

      case 'timeout_warning':
        console.log('‚è∞ Timeout warning received:', message);
        // Make sure the AI message is added to transcript if it has content
        if (message.message?.content) {
          this.transcript.push({
            type: 'assistant_message',
            content: message.message.content,
            timestamp: new Date().toISOString()
          });
        }
        this.emit('timeout_warning', message);
        break;
        
      case 'session_ended':
        console.log('‚è∞ Session ended by timeout:', message);
        // Make sure any final message is captured
        if (message.message?.content) {
          this.transcript.push({
            type: 'assistant_message', 
            content: message.message.content,
            timestamp: new Date().toISOString()
          });
        }
        this.emit('session_ended', message);
        break;
        
      case 'chat_metadata':
        console.log('üìä Chat metadata:', message.type, message);
        // Check if this contains timeout information
        if (message.type === 'chat_metadata' && message.metadata) {
          console.log('üìä Metadata details:', JSON.stringify(message.metadata, null, 2));
        }
        this.emit('chat_metadata', message);
        break;
        
      case 'assistant_end':
        console.log('üèÅ Assistant finished speaking');
        this.emit('audio_end'); // Treat as audio playback ended
        break;

      case 'tool_call':
        console.log('üîß Tool call received:', message);
        await this.handleToolCall(message);
        break;

      default:
        console.log('üì® Unknown message:', message.type, message);
    }
  }

  /**
   * Handle tool call from Hume EVI
   */
  private async handleToolCall(message: any): Promise<void> {
    try {
      console.log('üîß Processing tool call:', {
        toolCallId: message.tool_call_id,
        name: message.name,
        parameters: message.parameters
      });

      // Parse parameters from the tool call
      const parameters = JSON.parse(message.parameters);
      console.log('üì• Parsed parameters:', parameters);

      // Call backend get-answer endpoint
      const response = await fetch(`${this.baseUrl}/get-answer`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          user_id: parameters.user_id,
          question: parameters.question
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: 'Backend request failed' }));
        console.error('‚ùå Backend get-answer failed:', errorData);

        // Send tool error back to Hume
        await this.sendToolError(message.tool_call_id, errorData.error || 'Failed to get answer from backend');
        return;
      }

      const answerData = await response.json();
      console.log('‚úÖ Got answer from backend:', answerData);

      // Send tool response back to Hume
      await this.sendToolResponse(message.tool_call_id, answerData.answer || answerData.response || 'No answer provided');

    } catch (error) {
      console.error('‚ùå Error handling tool call:', error);
      await this.sendToolError(message.tool_call_id, `Tool call failed: ${error.message}`);
    }
  }

  /**
   * Send tool response back to Hume EVI
   */
  private async sendToolResponse(toolCallId: string, content: string): Promise<void> {
    if (!this.voiceClient) {
      console.error('‚ùå Cannot send tool response: no voice client');
      return;
    }

    try {
      const toolResponse = {
        type: 'tool_response',
        tool_call_id: toolCallId,
        content: content
      };

      console.log('üì§ Sending tool response:', toolResponse);

      // Use the voice client's sendToolMessage method
      await this.voiceClient.sendToolMessage(toolResponse);
      console.log('‚úÖ Tool response sent successfully');

    } catch (error) {
      console.error('‚ùå Error sending tool response:', error);
      // Try to send an error message instead
      await this.sendToolError(toolCallId, 'Failed to send tool response');
    }
  }

  /**
   * Send tool error back to Hume EVI
   */
  private async sendToolError(toolCallId: string, errorMessage: string): Promise<void> {
    if (!this.voiceClient) {
      console.error('‚ùå Cannot send tool error: no voice client');
      return;
    }

    try {
      const toolError = {
        type: 'tool_error',
        tool_call_id: toolCallId,
        error: errorMessage,
        code: 'TOOL_EXECUTION_ERROR',
        level: 'error',
        content: `I apologize, but I encountered an error: ${errorMessage}`
      };

      console.log('üì§ Sending tool error:', toolError);

      // Use the voice client's sendToolMessage method
      await this.voiceClient.sendToolMessage(toolError);
      console.log('‚úÖ Tool error sent successfully');

    } catch (error) {
      console.error('‚ùå Error sending tool error:', error);
    }
  }

  /**
   * Initialize Hume's audio player
   */
  private async initializeAudioPlayer(): Promise<void> {
    try {
      console.log('üîä Initializing Hume audio player...');
      this.audioPlayer = new EVIWebAudioPlayer();
      await this.audioPlayer.init();

      // Set up audio end detection ONLY ONCE when player is initialized
      if (!this.audioEndHandlerAttached) {
        this.audioPlayer.on('end', () => {
          console.log('‚úÖ AI audio playback completed');
          this.emit('audio_end'); // Signal audio playback ended
        });
        this.audioEndHandlerAttached = true;
        console.log('‚úÖ Audio end handler attached');
      }

      console.log('‚úÖ Hume audio player initialized');
    } catch (error) {
      console.error('‚ùå Error initializing audio player:', error);
      throw error;
    }
  }

  /**
   * Play audio output from Hume
   */
  private async playAudioOutput(message: any): Promise<void> {
    try {
      if (!this.audioPlayer) {
        console.warn('‚ö†Ô∏è Audio player not initialized');
        return;
      }

      console.log('üîä Playing AI audio via Hume player...');
      this.emit('audio_start'); // Signal audio playback started

      // Use Hume's proper audio player to handle streaming audio
      await this.audioPlayer.enqueue(message);

      // Event handler is already attached in initializeAudioPlayer
      // No need to add it again here

      console.log('‚úÖ AI audio enqueued for playback');
      
    } catch (error) {
      console.error('‚ùå Error playing audio:', error);
      this.emit('audio_end'); // Ensure we reset state on error
    }
  }

  /**
   * Start audio recording with MediaRecorder
   */
  private async startAudioRecording(): Promise<void> {
    console.log('üé§ Starting audio recording...');

    if (!this.mediaStream) {
      throw new Error('No media stream available');
    }

    if (!this.voiceClient) {
      throw new Error('Voice client not connected');
    }

    // Set up MediaRecorder
    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
      ? 'audio/webm;codecs=opus'
      : 'audio/webm';

    console.log('üé§ Using MIME type:', mimeType);
    this.recorder = new MediaRecorder(this.mediaStream, { mimeType });

    // Handle audio data
    this.recorder.ondataavailable = async (event: BlobEvent) => {
      if (event.data.size > 0 && this.voiceClient) {
        try {
          // Convert Blob to ArrayBuffer
          const arrayBuffer = await event.data.arrayBuffer();

          // Send as Uint8Array which is what Hume expects
          const audioData = new Uint8Array(arrayBuffer);

          // Send audio to Hume
          this.voiceClient.sendAudio(audioData);
          console.log('üé§ Audio chunk sent - size:', audioData.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error sending audio:', error);
        }
      }
    };

    this.recorder.onstart = () => {
      console.log('üé§ MediaRecorder started');
      this.isRecording = true;
    };

    this.recorder.onerror = (error) => {
      console.error('‚ùå MediaRecorder error:', error);
    };

    // Start recording with 100ms chunks
    this.recorder.start(100);
    console.log('‚úÖ Audio recording started');
  }

  /**
   * Start recording (for compatibility with UI)
   */
  async startRecording(): Promise<void> {
    console.log('üé§ startRecording called');
    // Recording is already started in the 'open' event handler
    // This method is kept for UI compatibility
  }

  /**
   * Stop recording
   */
  stopRecording(): void {
    if (this.recorder && this.isRecording) {
      this.recorder.stop();
      this.isRecording = false;
      console.log('üõë Recording stopped');
    }
  }

  /**
   * End interview
   */
  async endInterview(providedSessionId?: string, providedTranscript?: EVIMessage[]): Promise<{ transcript: EVIMessage[]; sessionId: string }> {
    console.log('üèÅ endInterview called. Current state:', {
      currentSession: !!this.currentSession,
      voiceClient: !!this.voiceClient,
      isRecording: this.isRecording,
      isEnding: this.isEnding,
      transcriptLength: this.transcript.length,
      providedSessionId,
      providedTranscriptLength: providedTranscript?.length || 0
    });

    if (this.isEnding) {
      console.warn('‚ö†Ô∏è Interview is already being ended, skipping duplicate call');
      return {
        transcript: providedTranscript || this.transcript,
        sessionId: providedSessionId || this.currentSession?.sessionId || ''
      };
    }

    // Use provided transcript if available
    if (providedTranscript && providedTranscript.length > 0) {
      this.transcript = providedTranscript;
      console.log('üìã Using provided transcript with', providedTranscript.length, 'messages');
    }

    // Handle case where we don't have a current session but have a session ID
    if (!this.currentSession && providedSessionId) {
      console.log('‚ö†Ô∏è No current session, but have provided session ID, creating minimal session data');
      // Return without trying to access null session
      return {
        transcript: this.transcript,
        sessionId: providedSessionId
      };
    }

    if (!this.currentSession) {
      console.error('‚ùå No active session found when ending interview');
      throw new Error('No active session');
    }

    this.isEnding = true;

    try {
      console.log('üèÅ Ending interview...');

      // SDK will stop audio handling on disconnect
      this.stopRecording();
      
      // Gracefully disconnect voice client
      if (this.voiceClient) {
        console.log('üîå Checking voice client connection state...');
        try {
          // Check if WebSocket is already closed (readyState 3 means CLOSED)
          // We need to check the internal WebSocket state if possible
          const socket = (this.voiceClient as any).socket || (this.voiceClient as any)._socket || (this.voiceClient as any).ws;
          if (socket && socket.readyState === 3) {
            console.log('‚ö†Ô∏è WebSocket already closed, skipping disconnect');
          } else {
            console.log('üîå Disconnecting voice client...');
            await this.voiceClient.disconnect();
            console.log('‚úÖ Voice client disconnected');
          }
        } catch (error) {
          console.warn('‚ö†Ô∏è Error disconnecting voice client:', error);
          // Continue anyway - the connection might already be closed
        }
      }

      // Skip saving transcript here - it will be saved by the complete endpoint
      console.log('üíæ Transcript will be saved by complete endpoint');
      const finalSessionId = providedSessionId || this.currentSession?.sessionId;

      const result = {
        transcript: this.transcript,
        sessionId: finalSessionId || ''
      };

      this.cleanup();
      console.log('‚úÖ Interview completed');
      
      return result;
      
    } catch (error) {
      console.error('‚ùå Error ending interview:', error);
      throw error;
    }
  }

  /**
   * Register event handler
   */
  onMessage(eventType: string, handler: Function): void {
    this.messageHandlers.set(eventType, handler);
  }

  /**
   * Emit event
   */
  private emit(eventType: string, data?: any): void {
    const handler = this.messageHandlers.get(eventType);
    if (handler) {
      handler(data);
    }
  }

  /**
   * Get current session
   */
  getCurrentSession(): EVISessionData | null {
    return this.currentSession;
  }

  /**
   * Get transcript
   */
  getTranscript(): EVIMessage[] {
    return [...this.transcript];
  }

  /**
   * Check if recording
   */
  isCurrentlyRecording(): boolean {
    return this.isRecording;
  }

  /**
   * Check if properly connected and ready
   */
  isConnectedAndReady(): boolean {
    // Don't require audioPlayer since it's initialized on first AI audio
    const ready = !!(this.voiceClient && this.mediaStream && this.currentSession);
    console.log('üîç isConnectedAndReady check:', {
      voiceClient: !!this.voiceClient,
      mediaStream: !!this.mediaStream,
      audioPlayer: !!this.audioPlayer,
      currentSession: !!this.currentSession,
      ready
    });
    return ready;
  }

  /**
   * Cleanup
   */
  cleanup(): void {
    // Prevent multiple cleanup calls
    if (!this.voiceClient && !this.mediaStream && !this.currentSession) {
      console.log('üîÑ Cleanup already completed, skipping');
      return;
    }
    
    console.log('üßπ Cleaning up...');
    
    try {
      this.stopRecording();
      
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop());
        this.mediaStream = null;
      }
      
      // Cleanup audio player
      if (this.audioPlayer) {
        try {
          // Remove event listeners before disposal
          if (this.audioEndHandlerAttached) {
            this.audioPlayer.off('end');
            this.audioEndHandlerAttached = false;
            console.log('üßπ Removed audio end event listener');
          }

          // Stop any playing audio before disposal
          try {
            this.audioPlayer.stop();
          } catch (e) {
            // Ignore stop errors
          }

          this.audioPlayer.dispose();
        } catch (e) {
          // Ignore dispose errors
        }
        this.audioPlayer = null;
      }
      
      // Disconnect without triggering events to prevent loops
      if (this.voiceClient) {
        const client = this.voiceClient;
        this.voiceClient = null; // Clear reference first
        try {
          client.disconnect();
        } catch (e) {
          // Ignore disconnect errors during cleanup
        }
      }
      
      this.currentSession = null;
      this.isConnecting = false;
      this.isRecording = false;
      this.isEnding = false;
      this.transcript = [];
      this.messageHandlers.clear();
      
      console.log('‚úÖ Cleanup complete');
      
    } catch (error) {
      console.error('‚ùå Error during cleanup:', error);
    }
  }
}

// Export singleton
export const directHumeEVI = new DirectHumeEVI();
export default DirectHumeEVI;